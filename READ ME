...Introduction....
The project here is an LLM Dashboard. Using it users will be able to select a source, 
ask questions about the soucre, recieve answers, and check chat history.

-------------------------------
....Overall Setup....

Notes for ollama:
Download ollama from site: https://ollama.com/download
If done correctly then inputting 'ollama' into the terminal should print the list of commands for ollama.
In Models choose Llama 3.1 8b: https://ollama.com/library/llama3.1:8b

VSCode:
In terminal for vscode write: python3 -m venv venv (this makes the virtual environment) <<<This was already done
To activate: 
    --> Windows: venv/scripts/activate.bat <<<This was already done
    --> Mac: source venv/bin/activate
To install streamlit: pip ollama streamlit <<<This was already done

-------------------------------
....Other Notes....

Created Bob.py
Make sure to select python virtual environment, it will be the venv one
Run ollama serve in other terminal to make sure ollama is running. 

To run: in terminal do: streamlit run Bob.py
